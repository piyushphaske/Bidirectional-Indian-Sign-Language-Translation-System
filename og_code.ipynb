{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1b539-e17f-4146-8c73-04f43a9891cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall googletrans\n",
    "!pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d0ebdbe-2bab-4def-9632-2c1ab0a0505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def add_visualizations(raw_sentence, processed_sentence, marathi_text=None, confidence_scores=None, processing_times=None):\n",
    "    \"\"\"\n",
    "    Generate visualizations after translation task execution\n",
    "    \n",
    "    Parameters:\n",
    "    - raw_sentence: The raw detected sentence from sign language\n",
    "    - processed_sentence: The corrected/processed sentence\n",
    "    - marathi_text: Translated Marathi text (optional)\n",
    "    - confidence_scores: Dictionary of {letter: confidence_score} (optional)\n",
    "    - processing_times: Dictionary with processing time metrics (optional)\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots for all visualizations\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # 1. Text Processing Comparison Visualization\n",
    "    plt.subplot(2, 3, 1)\n",
    "    text_processing_visualization(raw_sentence, processed_sentence)\n",
    "    \n",
    "    # 2. Translation Confidence Visualization\n",
    "    plt.subplot(2, 3, 2)\n",
    "    if confidence_scores:\n",
    "        letter_confidence_visualization(confidence_scores)\n",
    "    else:\n",
    "        # Generate dummy data if not provided\n",
    "        dummy_confidence = {letter: np.random.uniform(0.7, 1.0) for letter in set(raw_sentence.upper())}\n",
    "        letter_confidence_visualization(dummy_confidence)\n",
    "    \n",
    "    # 3. Word Cloud Visualization\n",
    "    plt.subplot(2, 3, 3)\n",
    "    generate_word_cloud(processed_sentence)\n",
    "    \n",
    "    # 4. Processing Time Breakdown\n",
    "    plt.subplot(2, 3, 4)\n",
    "    if processing_times:\n",
    "        processing_time_visualization(processing_times)\n",
    "    else:\n",
    "        # Generate dummy data if not provided\n",
    "        dummy_times = {\n",
    "            \"Detection\": np.random.uniform(0.1, 0.5),\n",
    "            \"Text Processing\": np.random.uniform(0.05, 0.2),\n",
    "            \"Translation\": np.random.uniform(0.2, 0.8),\n",
    "            \"TTS\": np.random.uniform(0.1, 0.3)\n",
    "        }\n",
    "        processing_time_visualization(dummy_times)\n",
    "    \n",
    "    # 5. Character Frequency Visualization\n",
    "    plt.subplot(2, 3, 5)\n",
    "    character_frequency_visualization(raw_sentence)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the visualizations\n",
    "    plt.savefig(\"translation_visualizations.png\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Display the visualizations\n",
    "    plt.show()\n",
    "    \n",
    "    # Return image for potential further use\n",
    "    img_buf = io.BytesIO()\n",
    "    plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buf.seek(0)\n",
    "    return Image.open(img_buf)\n",
    "\n",
    "\n",
    "def text_processing_visualization(raw_sentence, processed_sentence):\n",
    "    \"\"\"Visualization 1: Compare raw and processed text\"\"\"\n",
    "    # Create a DataFrame for comparison\n",
    "    raw_words = raw_sentence.split()\n",
    "    processed_words = processed_sentence.split()\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    word_pairs = []\n",
    "    for i in range(max(len(raw_words), len(processed_words))):\n",
    "        raw = raw_words[i] if i < len(raw_words) else \"\"\n",
    "        processed = processed_words[i] if i < len(processed_words) else \"\"\n",
    "        word_pairs.append((raw, processed))\n",
    "    \n",
    "    # Create a figure for comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Extract raw and processed words\n",
    "    raw_words_plot = [pair[0] for pair in word_pairs]\n",
    "    processed_words_plot = [pair[1] for pair in word_pairs]\n",
    "    \n",
    "    # Create a barplot for comparison\n",
    "    x = np.arange(len(word_pairs))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Plot bars for raw and processed words (just for visual, not quantitative)\n",
    "    ax.bar(x - width/2, [len(word) for word in raw_words_plot], width, label='Raw Text', color='lightcoral')\n",
    "    ax.bar(x + width/2, [len(word) for word in processed_words_plot], width, label='Processed Text', color='lightseagreen')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_title('Raw vs Processed Text Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f\"{raw}\\nâ†’\\n{proc}\" for raw, proc in word_pairs], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Show changes in text\n",
    "    plt.figtext(0.5, 0.01, f\"Raw: '{raw_sentence}'\\nProcessed: '{processed_sentence}'\", \n",
    "                ha='center', fontsize=10, bbox=dict(facecolor='lightgray', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def letter_confidence_visualization(confidence_scores):\n",
    "    \"\"\"Visualization 2: Letter detection confidence\"\"\"\n",
    "    # Sort letters by confidence score\n",
    "    sorted_letters = sorted(confidence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    letters = [item[0] for item in sorted_letters]\n",
    "    scores = [item[1] for item in sorted_letters]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    plt.barh(letters, scores, color='skyblue')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Detected Letter')\n",
    "    plt.title('Letter Detection Confidence')\n",
    "    plt.xlim(0, 1.0)\n",
    "    \n",
    "    # Add confidence threshold line\n",
    "    plt.axvline(x=0.7, color='red', linestyle='--', label='Threshold (0.7)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add average confidence score text\n",
    "    avg_score = sum(scores) / len(scores)\n",
    "    plt.figtext(0.7, 0.8, f\"Average Confidence: {avg_score:.2f}\", \n",
    "                backgroundcolor='white', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def generate_word_cloud(text):\n",
    "    \"\"\"Visualization 3: Word cloud of processed text\"\"\"\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                         max_words=50, contour_width=3, contour_color='steelblue').generate(text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Processed Text')\n",
    "\n",
    "\n",
    "def processing_time_visualization(processing_times):\n",
    "    \"\"\"Visualization 4: Processing time breakdown\"\"\"\n",
    "    # Create a pie chart for processing time breakdown\n",
    "    labels = list(processing_times.keys())\n",
    "    sizes = list(processing_times.values())\n",
    "    \n",
    "    # Calculate total processing time\n",
    "    total_time = sum(sizes)\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, \n",
    "            shadow=True, explode=[0.05] * len(labels),\n",
    "            colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.title(f'Processing Time Breakdown (Total: {total_time:.2f}s)')\n",
    "\n",
    "\n",
    "def character_frequency_visualization(text):\n",
    "    \"\"\"Visualization 5: Character frequency in detected text\"\"\"\n",
    "    # Clean the text - keep only letters\n",
    "    cleaned_text = ''.join(char.upper() for char in text if char.isalpha())\n",
    "    \n",
    "    # Count character frequency\n",
    "    char_count = Counter(cleaned_text)\n",
    "    \n",
    "    # Sort by frequency\n",
    "    chars = sorted(char_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract characters and their counts\n",
    "    characters = [item[0] for item in chars]\n",
    "    counts = [item[1] for item in chars]\n",
    "    \n",
    "    # Create bar chart\n",
    "    plt.bar(characters, counts, color='mediumseagreen')\n",
    "    plt.xlabel('Character')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Character Frequency in Detected Text')\n",
    "    \n",
    "    # Add total count text\n",
    "    total_chars = sum(counts)\n",
    "    plt.figtext(0.7, 0.8, f\"Total Characters: {total_chars}\", \n",
    "                backgroundcolor='white', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "# Bonus animation visualization - can be used as a standalone visualization\n",
    "def create_translation_animation(text, confidence_scores=None, output_path=\"translation_animation.mp4\"):\n",
    "    \"\"\"\n",
    "    Create an animation showing the letters being detected in sequence\n",
    "    \n",
    "    Parameters:\n",
    "    - text: The text to animate\n",
    "    - confidence_scores: Dictionary of {letter: confidence_score}\n",
    "    - output_path: Path to save the animation video\n",
    "    \"\"\"\n",
    "    # Generate random confidence scores if not provided\n",
    "    if not confidence_scores:\n",
    "        confidence_scores = {letter: np.random.uniform(0.7, 1.0) for letter in text}\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        \n",
    "        # Show detected text so far\n",
    "        detected_text = text[:frame+1]\n",
    "        remaining_text = text[frame+1:] if frame+1 < len(text) else \"\"\n",
    "        \n",
    "        # Set up the plot\n",
    "        ax.text(0.1, 0.5, detected_text, fontsize=24, color='blue')\n",
    "        ax.text(0.1 + len(detected_text) * 0.025, 0.5, remaining_text, fontsize=24, color='gray', alpha=0.5)\n",
    "        \n",
    "        # Show confidence for current letter\n",
    "        if frame < len(text):\n",
    "            current_letter = text[frame]\n",
    "            confidence = confidence_scores.get(current_letter, 0.8)\n",
    "            ax.text(0.1, 0.3, f\"Detecting: {current_letter}\", fontsize=16)\n",
    "            ax.text(0.1, 0.2, f\"Confidence: {confidence:.2f}\", fontsize=16)\n",
    "            \n",
    "            # Add a confidence bar\n",
    "            ax.barh(0.1, confidence, height=0.05, color='green')\n",
    "            ax.barh(0.1, 1.0, height=0.05, color='lightgray', alpha=0.3)\n",
    "            \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Sign Language Detection Animation')\n",
    "        \n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=len(text) + 10, interval=500)\n",
    "    \n",
    "    # Save animation\n",
    "    anim.save(output_path, writer='ffmpeg', fps=2, dpi=100)\n",
    "    \n",
    "    plt.close()\n",
    "    return output_path\n",
    "\n",
    "\n",
    "# Real-time visualization for webcam input\n",
    "def add_real_time_visualization(frame, detected_letter, confidence, current_word, sentence):\n",
    "    \"\"\"\n",
    "    Add real-time visualization elements to the camera frame\n",
    "    \n",
    "    Parameters:\n",
    "    - frame: The current camera frame\n",
    "    - detected_letter: Currently detected letter\n",
    "    - confidence: Confidence score for the detection\n",
    "    - current_word: Current word being formed\n",
    "    - sentence: Complete sentence formed so far\n",
    "    \n",
    "    Returns:\n",
    "    - Modified frame with visualizations\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Create a semi-transparent overlay for visualizations\n",
    "    overlay = frame.copy()\n",
    "    \n",
    "    # Draw detection confidence bar\n",
    "    if detected_letter is not None:\n",
    "        # Background bar\n",
    "        cv2.rectangle(overlay, (w-220, 20), (w-20, 50), (200, 200, 200), -1)\n",
    "        # Confidence level\n",
    "        conf_width = int(200 * confidence)\n",
    "        cv2.rectangle(overlay, (w-220, 20), (w-220+conf_width, 50), (0, 255, 0), -1)\n",
    "        # Text\n",
    "        cv2.putText(overlay, f\"{detected_letter}: {confidence:.2f}\", \n",
    "                   (w-210, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "    \n",
    "    # Create bottom info panel\n",
    "    cv2.rectangle(overlay, (0, h-150), (w, h), (240, 240, 240), -1)\n",
    "    \n",
    "    # Current word with letter-by-letter color coding\n",
    "    word_x = 20\n",
    "    word_y = h-110\n",
    "    cv2.putText(overlay, \"Current Word:\", (word_x, word_y-30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    \n",
    "    for i, letter in enumerate(current_word):\n",
    "        color = (0, 100, 200)  # Blue for letters in the word\n",
    "        cv2.putText(overlay, letter, (word_x + i*30, word_y), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 2)\n",
    "    \n",
    "    # Sentence so far\n",
    "    cv2.putText(overlay, \"Sentence:\", (20, h-60), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    \n",
    "    # Display sentence with word wrapping\n",
    "    max_width = w - 40\n",
    "    words = sentence.split()\n",
    "    line = \"\"\n",
    "    line_y = h-30\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = line + word + \" \"\n",
    "        # Check if adding word would exceed frame width\n",
    "        text_size = cv2.getTextSize(test_line, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]\n",
    "        if text_size[0] > max_width:\n",
    "            cv2.putText(overlay, line, (20, line_y), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "            line = word + \" \"\n",
    "            line_y += 30\n",
    "        else:\n",
    "            line = test_line\n",
    "    \n",
    "    # Print the last line\n",
    "    cv2.putText(overlay, line, (20, line_y), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "    \n",
    "    # Combine the frame and overlay\n",
    "    alpha = 0.7\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0, frame)\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b111b158-0b67-43cc-90c5-6af5c3aaa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_system_metrics(test_data=None):\n",
    "    \"\"\"\n",
    "    Calculate and display various evaluation metrics for the sign language detection system\n",
    "    \n",
    "    Parameters:\n",
    "    - test_data: Optional test data to evaluate against. If None, will use sample data or run a test.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of metrics\n",
    "    \"\"\"\n",
    "    # If no test data is provided, use sample data for demonstration\n",
    "    if test_data is None:\n",
    "        print(\"No test data provided. Using sample evaluation data...\")\n",
    "        # Sample data - in a real system, this would come from evaluation runs\n",
    "        y_true = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "                  'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "        \n",
    "        # Simulate predictions with some errors\n",
    "        y_pred = ['A', 'B', 'C', 'D', 'F', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'N', \n",
    "                  'N', 'O', 'R', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "        \n",
    "        # Detection times (seconds)\n",
    "        detection_times = np.random.uniform(0.05, 0.2, len(y_true))\n",
    "        \n",
    "        # Confidence scores (0-1)\n",
    "        confidence_scores = np.random.uniform(0.7, 0.99, len(y_true))\n",
    "    else:\n",
    "        # Use provided test data\n",
    "        y_true = test_data['true_labels']\n",
    "        y_pred = test_data['predicted_labels']\n",
    "        detection_times = test_data['detection_times']\n",
    "        confidence_scores = test_data['confidence_scores']\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    avg_detection_time = np.mean(detection_times)\n",
    "    avg_confidence = np.mean(confidence_scores)\n",
    "    \n",
    "    # Error rate\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    # Calculate per-letter accuracy\n",
    "    letter_metrics = {}\n",
    "    unique_letters = sorted(set(y_true))\n",
    "    \n",
    "    for letter in unique_letters:\n",
    "        indices = [i for i, l in enumerate(y_true) if l == letter]\n",
    "        correct = sum(1 for i in indices if y_pred[i] == y_true[i])\n",
    "        total = len(indices)\n",
    "        letter_accuracy = correct / total if total > 0 else 0\n",
    "        avg_letter_conf = np.mean([confidence_scores[i] for i in indices]) if indices else 0\n",
    "        letter_metrics[letter] = {\n",
    "            'accuracy': letter_accuracy,\n",
    "            'avg_confidence': avg_letter_conf,\n",
    "            'sample_count': total\n",
    "        }\n",
    "    \n",
    "    # Compile all metrics\n",
    "    all_metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'error_rate': error_rate,\n",
    "        'avg_detection_time': avg_detection_time,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'confusion_matrix': cm,\n",
    "        'letter_metrics': letter_metrics\n",
    "    }\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def display_evaluation_metrics(metrics=None):\n",
    "    \"\"\"\n",
    "    Display system evaluation metrics with visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    - metrics: Dictionary of metrics. If None, will calculate metrics.\n",
    "    \"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = calculate_system_metrics()\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # 1. Overall metrics bar chart\n",
    "    plt.subplot(2, 3, 1)\n",
    "    overall_metrics = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1_score']]\n",
    "    metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    \n",
    "    plt.bar(metric_labels, overall_metrics, color=['steelblue', 'forestgreen', 'darkorange', 'purple'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title('Overall System Performance')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add text labels on bars\n",
    "    for i, v in enumerate(overall_metrics):\n",
    "        plt.text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Confusion Matrix Heatmap\n",
    "    plt.subplot(2, 3, 2)\n",
    "    cm = metrics['confusion_matrix']\n",
    "    unique_letters = sorted(metrics['letter_metrics'].keys())\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_letters, yticklabels=unique_letters)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    # 3. Per-letter Accuracy\n",
    "    plt.subplot(2, 3, 3)\n",
    "    letters = list(metrics['letter_metrics'].keys())\n",
    "    letter_accuracies = [metrics['letter_metrics'][l]['accuracy'] for l in letters]\n",
    "    \n",
    "    # Create gradient colors based on accuracy\n",
    "    colors = plt.cm.RdYlGn(np.array(letter_accuracies))\n",
    "    \n",
    "    plt.bar(letters, letter_accuracies, color=colors)\n",
    "    plt.axhline(y=metrics['accuracy'], color='r', linestyle='--', label=f'Overall Accuracy: {metrics[\"accuracy\"]:.3f}')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Per-Letter Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 4. Detection Time vs Confidence Analysis\n",
    "    plt.subplot(2, 3, 4)\n",
    "    letter_times = []\n",
    "    letter_confs = []\n",
    "    sizes = []\n",
    "    \n",
    "    for letter in letters:\n",
    "        metrics_data = metrics['letter_metrics'][letter]\n",
    "        letter_times.append(metrics['avg_detection_time'])  # Using overall average as example\n",
    "        letter_confs.append(metrics_data['avg_confidence'])\n",
    "        sizes.append(metrics_data['sample_count'] * 20)  # Scale by sample count\n",
    "    \n",
    "    plt.scatter(letter_times, letter_confs, s=sizes, alpha=0.6, c=letter_accuracies, cmap='viridis')\n",
    "    \n",
    "    for i, letter in enumerate(letters):\n",
    "        plt.annotate(letter, (letter_times[i], letter_confs[i]), fontsize=9)\n",
    "    \n",
    "    plt.title('Detection Time vs. Confidence')\n",
    "    plt.xlabel('Detection Time (s)')\n",
    "    plt.ylabel('Confidence Score')\n",
    "    plt.colorbar(label='Accuracy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. System Performance Summary\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    System Performance Summary:\n",
    "    \n",
    "    â€¢ Overall Accuracy: {metrics['accuracy']:.3f}\n",
    "    â€¢ Precision: {metrics['precision']:.3f}\n",
    "    â€¢ Recall: {metrics['recall']:.3f}\n",
    "    â€¢ F1 Score: {metrics['f1_score']:.3f}\n",
    "    â€¢ Error Rate: {metrics['error_rate']:.3f}\n",
    "    \n",
    "    Speed Metrics:\n",
    "    â€¢ Avg. Detection Time: {metrics['avg_detection_time']:.3f}s\n",
    "    \n",
    "    Confidence Metrics:\n",
    "    â€¢ Avg. Confidence Score: {metrics['avg_confidence']:.3f}\n",
    "    \n",
    "    Most Accurate Letters:\n",
    "    {get_top_n_letters(metrics, 'accuracy', 3)}\n",
    "    \n",
    "    Least Accurate Letters:\n",
    "    {get_top_n_letters(metrics, 'accuracy', 3, bottom=True)}\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.text(0, 1, summary_text, fontsize=12, va='top')\n",
    "    \n",
    "    # 6. Word-level Performance Estimation (Simulated)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    \n",
    "    # Simulate word accuracy based on letter accuracy\n",
    "    # Word accuracy typically decreases with word length due to compounding errors\n",
    "    word_lengths = list(range(1, 11))\n",
    "    word_accuracies = [metrics['accuracy'] ** length for length in word_lengths]\n",
    "    \n",
    "    plt.plot(word_lengths, word_accuracies, 'o-', color='teal')\n",
    "    plt.title('Estimated Word Recognition Accuracy')\n",
    "    plt.xlabel('Word Length (characters)')\n",
    "    plt.ylabel('Estimated Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(word_lengths)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"system_evaluation_metrics.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print out key metrics to console\n",
    "    print(\"\\nSYSTEM EVALUATION METRICS SUMMARY:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.3f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n",
    "    print(f\"Average Detection Time: {metrics['avg_detection_time']:.3f} seconds\")\n",
    "    print(f\"Average Confidence Score: {metrics['avg_confidence']:.3f}\")\n",
    "\n",
    "def get_top_n_letters(metrics, metric_name, n=3, bottom=False):\n",
    "    \"\"\"Helper function to get top or bottom N letters based on a metric\"\"\"\n",
    "    letters = list(metrics['letter_metrics'].keys())\n",
    "    metric_values = [metrics['letter_metrics'][l][metric_name] for l in letters]\n",
    "    \n",
    "    # Sort letters based on metric values\n",
    "    sorted_indices = np.argsort(metric_values)\n",
    "    if not bottom:\n",
    "        sorted_indices = sorted_indices[::-1]  # Reverse for top N\n",
    "    \n",
    "    result = \"\"\n",
    "    for i in range(min(n, len(letters))):\n",
    "        idx = sorted_indices[i]\n",
    "        result += f\"â€¢ {letters[idx]}: {metric_values[idx]:.3f}\\n\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def collect_test_data(test_video_path=None, num_samples=None):\n",
    "    \"\"\"\n",
    "    Collect real test data by processing a known test video or sample data\n",
    "    \n",
    "    Parameters:\n",
    "    - test_video_path: Path to a test video with known ground truth\n",
    "    - num_samples: Number of samples to process (if None, process all)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with test data metrics\n",
    "    \"\"\"\n",
    "    print(\"This function would normally gather real test data from a video with known ground truth.\")\n",
    "    print(\"For demonstration purposes, we're using simulated test data.\")\n",
    "    \n",
    "    # In a real implementation, this would process actual test videos and compare with ground truth\n",
    "    # Simulated test data\n",
    "    test_data = {\n",
    "        'true_labels': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', \n",
    "                        'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n",
    "                        'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
    "        'predicted_labels': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', \n",
    "                            'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n",
    "                            'U', 'V', 'W', 'X', 'Z', 'Y'],  # Added some errors\n",
    "        'detection_times': np.random.uniform(0.05, 0.2, 26),\n",
    "        'confidence_scores': np.random.uniform(0.7, 0.95, 26)\n",
    "    }\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b8896e6-073b-42f4-a5cd-da08ee4e4b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'httpcore' has no attribute 'SyncHTTPTransport'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# from nltk.translate.bleu_score import sentence_bleu\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# from nltk.metrics import edit_distance\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgtts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gTTS\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\googletrans\\__init__.py:6\u001b[0m\n\u001b[0;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.1.0-alpha\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANGCODES, LANGUAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\googletrans\\client.py:27\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translated, Detected\n\u001b[0;32m     24\u001b[0m EXCLUDES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mca\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTranslator\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Google Translate ajax API implementation class\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;43;03m    You have to create an instance of Translator to use this API\u001b[39;49;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;43;03m    :type raise_exception: boolean\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_urls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_CLIENT_SERVICE_URLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_USER_AGENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_RAISE_EXCEPTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttpcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncHTTPTransport\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\googletrans\\client.py:57\u001b[0m, in \u001b[0;36mTranslator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Google Translate ajax API implementation class\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    You have to create an instance of Translator to use this API\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    :type raise_exception: boolean\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, service_urls\u001b[38;5;241m=\u001b[39mDEFAULT_CLIENT_SERVICE_URLS, user_agent\u001b[38;5;241m=\u001b[39mDEFAULT_USER_AGENT,\n\u001b[0;32m     56\u001b[0m                  raise_exception\u001b[38;5;241m=\u001b[39mDEFAULT_RAISE_EXCEPTION,\n\u001b[1;32m---> 57\u001b[0m                  proxies: typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, \u001b[43mhttpcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncHTTPTransport\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m                  timeout: Timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m                  http2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(http2\u001b[38;5;241m=\u001b[39mhttp2)\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m proxies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'httpcore' has no attribute 'SyncHTTPTransport'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import warnings\n",
    "from ultralytics import YOLO\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from IPython.display import Video, display\n",
    "import moviepy.editor as mpy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from nltk.metrics import edit_distance\n",
    "from googletrans import Translator\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "from io import BytesIO\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from PIL import Image\n",
    "import io\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the trained YOLO model\n",
    "model = YOLO(r'train5/weights/last.pt')\n",
    "model.to('cpu')\n",
    "\n",
    "# Generate a color map for classes\n",
    "np.random.seed(42)  # for reproducibility\n",
    "color_map = {cls: tuple(map(int, np.random.randint(0, 255, 3))) for cls in range(len(model.names))}\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def correct_sentence_nltk(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    corrected_tokens = []\n",
    "    for token, pos in pos_tags:\n",
    "        wordnet_pos = get_wordnet_pos(pos)\n",
    "        lemma = lemmatizer.lemmatize(token, wordnet_pos)\n",
    "        corrected_tokens.append(lemma)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    corrected_tokens = [word for word in corrected_tokens if word not in stop_words]\n",
    "    \n",
    "    corrected_sentence = ' '.join(corrected_tokens)\n",
    "    return corrected_sentence.capitalize()\n",
    "\n",
    "def process_video_from_camera():\n",
    "    current_word = \"\"\n",
    "    sentence = \"\"\n",
    "    last_detected_letter = None\n",
    "    letter_counter = 0\n",
    "    frame_counter = 0\n",
    "    LETTER_THRESHOLD = 10\n",
    "    WORD_FRAME_THRESHOLD = 15\n",
    "    MAX_RECORDING_TIME = 60  # Maximum recording time in seconds\n",
    "    \n",
    "    # Initialize dictionary to store letter confidences\n",
    "    letter_confidences = {}\n",
    "    \n",
    "    # Initialize camera\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default camera\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return \"\", \"\", letter_confidences\n",
    "    \n",
    "    print(\"Camera initialized successfully. Recording started...\")\n",
    "    print(\"Press 'q' to stop recording.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "        \n",
    "        # Check if maximum recording time has elapsed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time > MAX_RECORDING_TIME:\n",
    "            print(f\"Maximum recording time of {MAX_RECORDING_TIME} seconds reached.\")\n",
    "            break\n",
    "        \n",
    "        # Display elapsed time on frame\n",
    "        cv2.putText(frame, f\"Time: {int(elapsed_time)}s\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        results = model(frame)\n",
    "        detected_letter = None\n",
    "        conf = 0  # Initialize conf to avoid reference errors\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                cls = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                class_name = model.names[cls]\n",
    "\n",
    "                if class_name.isalpha() and len(class_name) == 1 and conf > 0.7:\n",
    "                    detected_letter = class_name.upper()\n",
    "                    # Store confidence score for visualization\n",
    "                    letter_confidences[detected_letter] = conf\n",
    "                    \n",
    "                    # Draw bounding box with color based on class\n",
    "                    color = color_map[cls]\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)  # Thickness of 3\n",
    "                    cv2.putText(frame, f\"{class_name} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "        # Add real-time visualization to the frame\n",
    "        frame = add_real_time_visualization(frame, detected_letter, conf if detected_letter else 0, \n",
    "                                           current_word, sentence)\n",
    "\n",
    "        # Display current word and sentence\n",
    "        cv2.putText(frame, f\"Current Word: {current_word}\", (10, 60), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"Sentence: {sentence}\", (10, 90), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Camera Input - Sign Language Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Recording stopped manually.\")\n",
    "            break\n",
    "\n",
    "        if detected_letter == last_detected_letter and detected_letter is not None:\n",
    "            letter_counter += 1\n",
    "            if letter_counter == LETTER_THRESHOLD:\n",
    "                current_word += detected_letter\n",
    "                letter_counter = 0\n",
    "                frame_counter = 0\n",
    "        else:\n",
    "            letter_counter = 0\n",
    "\n",
    "        frame_counter += 1\n",
    "\n",
    "        if current_word and frame_counter > WORD_FRAME_THRESHOLD:\n",
    "            sentence += current_word + \" \"\n",
    "            current_word = \"\"\n",
    "            frame_counter = 0\n",
    "\n",
    "        last_detected_letter = detected_letter\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    raw_sentence = sentence.strip()\n",
    "    print(f\"Raw detected sentence: {raw_sentence}\")\n",
    "    \n",
    "    corrected_sentence = correct_sentence_nltk(raw_sentence)\n",
    "    print(f\"Corrected sentence: {corrected_sentence}\")\n",
    "\n",
    "    return raw_sentence, corrected_sentence, letter_confidences\n",
    "    \n",
    "def process_video_from_file(video_path):\n",
    "    current_word = \"\"\n",
    "    sentence = \"\"\n",
    "    last_detected_letter = None\n",
    "    letter_counter = 0\n",
    "    frame_counter = 0\n",
    "    LETTER_THRESHOLD = 10\n",
    "    WORD_FRAME_THRESHOLD = 15\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model(frame)\n",
    "        detected_letter = None\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                cls = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                class_name = model.names[cls]\n",
    "\n",
    "                if class_name.isalpha() and len(class_name) == 1 and conf > 0.7:\n",
    "                    detected_letter = class_name.upper()\n",
    "                    \n",
    "                    # Draw bounding box with color based on class\n",
    "                    color = color_map[cls]\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)  # Thickness of 3\n",
    "                    cv2.putText(frame, f\"{class_name} {conf:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "        cv2.imshow('Video Processing', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if detected_letter == last_detected_letter and detected_letter is not None:\n",
    "            letter_counter += 1\n",
    "            if letter_counter == LETTER_THRESHOLD:\n",
    "                current_word += detected_letter\n",
    "                letter_counter = 0\n",
    "                frame_counter = 0\n",
    "        else:\n",
    "            letter_counter = 0\n",
    "\n",
    "        frame_counter += 1\n",
    "\n",
    "        if current_word and frame_counter > WORD_FRAME_THRESHOLD:\n",
    "            sentence += current_word + \" \"\n",
    "            current_word = \"\"\n",
    "            frame_counter = 0\n",
    "\n",
    "        last_detected_letter = detected_letter\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    raw_sentence = sentence.strip()\n",
    "    corrected_sentence = correct_sentence_nltk(raw_sentence)\n",
    "\n",
    "    return raw_sentence, corrected_sentence\n",
    "\n",
    "def translate_text(text, src_lang, dest_lang):\n",
    "    translator = Translator()\n",
    "    translated = translator.translate(text, src=src_lang, dest=dest_lang)\n",
    "    return translated.text\n",
    "\n",
    "def text_to_speech(text, language):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "    \n",
    "    tts = gTTS(text=text, lang=language)\n",
    "    fp = BytesIO()\n",
    "    tts.write_to_fp(fp)\n",
    "    fp.seek(0)\n",
    "    \n",
    "    pygame.mixer.music.load(fp)\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        pygame.time.Clock().tick(10)\n",
    "    \n",
    "    pygame.mixer.quit()\n",
    "    pygame.quit()\n",
    "\n",
    "def speech_to_text(language_code):\n",
    "    recognizer = sr.Recognizer()\n",
    "    microphone = sr.Microphone()\n",
    "    \n",
    "    with microphone as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(f\"Say something in {language_code.split('-')[0]}...\")\n",
    "        audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio, language=language_code)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, could not understand the audio.\")\n",
    "    except sr.RequestError:\n",
    "        print(\"Sorry, there was an error with the request.\")\n",
    "    return None\n",
    "def find_word_folder_case_insensitive(dataset_folder, word):\n",
    "    \"\"\"Find a folder matching the word, ignoring case.\"\"\"\n",
    "    word = word.lower()\n",
    "    # Check if the exact folder exists\n",
    "    if os.path.exists(os.path.join(dataset_folder, word)):\n",
    "        return os.path.join(dataset_folder, word)\n",
    "    \n",
    "    # Check case-insensitive\n",
    "    for folder in os.listdir(dataset_folder):\n",
    "        if folder.lower() == word:\n",
    "            return os.path.join(dataset_folder, folder)\n",
    "    \n",
    "    print(f\"No folder found for word: {word}\")\n",
    "    return None\n",
    "\n",
    "def find_and_concatenate_videos(sentence, dataset_folder, output_folder):\n",
    "    video_clips = []\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    \n",
    "    print(f\"Words to find videos for: {words}\")\n",
    "    \n",
    "    for word in words:\n",
    "        word_folder = find_word_folder_case_insensitive(dataset_folder, word)\n",
    "        if word_folder:\n",
    "            print(f\"Found folder for word '{word}': {word_folder}\")\n",
    "            video_files = [f for f in os.listdir(word_folder) if f.lower().endswith('.mp4')]\n",
    "            if video_files:\n",
    "                print(f\"Found video files: {video_files}\")\n",
    "                video_path = os.path.join(word_folder, video_files[0])\n",
    "                try:\n",
    "                    video_clips.append(VideoFileClip(video_path))\n",
    "                    print(f\"Added video: {video_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading video {video_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"No .mp4 files found in folder: {word_folder}\")\n",
    "        else:\n",
    "            print(f\"No folder found for word: {word}\")\n",
    "    \n",
    "    if video_clips:\n",
    "        print(f\"Total video clips found: {len(video_clips)}\")\n",
    "        try:\n",
    "            final_clip = concatenate_videoclips(video_clips, method=\"compose\")\n",
    "            output_video_path = os.path.join(output_folder, \"output_video.mp4\")\n",
    "            final_clip.write_videofile(output_video_path, codec=\"libx264\", fps=24)\n",
    "            print(f\"Successfully created video at: {output_video_path}\")\n",
    "            return output_video_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating videos: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No videos found for the sentence.\")\n",
    "        return None\n",
    "\n",
    "def find_char_folder_case_insensitive(dataset_path, char):\n",
    "    \"\"\"Find a folder matching the character, ignoring case.\"\"\"\n",
    "    char = char.upper()\n",
    "    # Check if the exact folder exists\n",
    "    if os.path.exists(os.path.join(dataset_path, char)):\n",
    "        return os.path.join(dataset_path, char)\n",
    "    \n",
    "    # Check case-insensitive\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        if folder.upper() == char:\n",
    "            return os.path.join(dataset_path, folder)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def get_image_path(char, dataset_path):\n",
    "    if not char.isalnum() and not char.isspace():\n",
    "        return None  # Skip punctuation and special characters\n",
    "        \n",
    "    if char.isspace():\n",
    "        # Handle spaces - you might want to add a blank image for spaces\n",
    "        return None\n",
    "        \n",
    "    char_folder = find_char_folder_case_insensitive(dataset_path, char)\n",
    "    if char_folder and os.path.exists(char_folder) and os.listdir(char_folder):\n",
    "        image_files = [f for f in os.listdir(char_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if image_files:\n",
    "            image_path = os.path.join(char_folder, image_files[0])\n",
    "            return image_path\n",
    "    \n",
    "    print(f\"No image found for character: {char}\")\n",
    "    return None\n",
    "\n",
    "def create_image_sequence(text, dataset_path):\n",
    "    image_paths = []\n",
    "    print(f\"Creating image sequence for text: {text}\")\n",
    "    \n",
    "    for char in text:\n",
    "        if char.isspace():\n",
    "            continue  # Skip spaces\n",
    "            \n",
    "        image_path = get_image_path(char, dataset_path)\n",
    "        if image_path:\n",
    "            print(f\"Found image for character '{char}': {image_path}\")\n",
    "            image_paths.append(image_path)\n",
    "        else:\n",
    "            print(f\"No image found for character: '{char}'\")\n",
    "    \n",
    "    print(f\"Total images found: {len(image_paths)}\")\n",
    "    return image_paths\n",
    "\n",
    "def create_video_from_images(image_paths, output_video_path, frame_duration=1):\n",
    "    if not image_paths:\n",
    "        print(\"No images provided to create video\")\n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        clips = [mpy.ImageClip(img).set_duration(frame_duration) for img in image_paths]\n",
    "        video = mpy.concatenate_videoclips(clips, method=\"compose\")\n",
    "        video.write_videofile(output_video_path, fps=24)\n",
    "        print(f\"Successfully created video at: {output_video_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating video from images: {e}\")\n",
    "        return False\n",
    "\n",
    "def video_from_image(text):\n",
    "    dataset_path = \"single_photo_dataset\"\n",
    "    sanitized_text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
    "    output_video_path = f\"output_video/{sanitized_text}.mp4\"\n",
    "    \n",
    "    if not os.path.exists(\"output_video\"):\n",
    "        os.makedirs(\"output_video\")\n",
    "    \n",
    "    image_paths = create_image_sequence(text, dataset_path)\n",
    "    if image_paths:\n",
    "        success = create_video_from_images(image_paths, output_video_path)\n",
    "        if success:\n",
    "            return output_video_path\n",
    "    \n",
    "    print(\"Failed to create video from images.\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(\"output_video\"):\n",
    "        os.makedirs(\"output_video\")\n",
    "    \n",
    "    # Hardcoded video file path for option 1\n",
    "    video_file_path = r\"C:\\Users\\Piyush\\NLP_mini_Project\\NLP_mini_Project\\example_video.mp4\"\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n1. Translate Hand Signs to Marathi Speech\")\n",
    "        print(\"2. Translate Text/Speech to Hand Signs\")\n",
    "        print(\"3. View Translation Visualizations\")\n",
    "        print(\"4. Create Translation Animation\")\n",
    "        print(\"5. Show System Evaluation Metrics\")\n",
    "        print(\"6. Exit\")\n",
    "        \n",
    "        choice = input(\"Enter your choice (1-6): \")\n",
    "\n",
    "        if choice == '1':\n",
    "            # Ask user to select input source\n",
    "            print(\"\\nSelect Input Source:\")\n",
    "            print(\"1. Camera Input\")\n",
    "            print(\"2. Video File Input\")\n",
    "            input_source = input(\"Enter your choice (1-2): \")\n",
    "            \n",
    "            # Record starting time\n",
    "            start_time = time.time()\n",
    "            detection_start = time.time()\n",
    "            \n",
    "            if input_source == '1':\n",
    "                # Use camera input\n",
    "                print(\"Using camera input for sign language detection...\")\n",
    "                raw_sentence, processed_sentence, letter_confidences = process_video_from_camera()\n",
    "            else:\n",
    "                # Use video file input\n",
    "                print(f\"Using video file: {video_file_path}\")\n",
    "                raw_sentence, processed_sentence = process_video_from_file(video_file_path)\n",
    "                letter_confidences = {}  # Empty dict as process_video_from_file doesn't return confidences\n",
    "            \n",
    "            # Record detection time\n",
    "            detection_time = time.time() - detection_start\n",
    "            processing_start = time.time()\n",
    "            \n",
    "            if processed_sentence:\n",
    "                print(\"Detected sentence:\", processed_sentence)\n",
    "                \n",
    "                # Record translation start time\n",
    "                translation_start = time.time()\n",
    "                marathi_text = translate_text(processed_sentence, 'en', 'mr')\n",
    "                translation_time = time.time() - translation_start\n",
    "                \n",
    "                print(\"Marathi translation:\", marathi_text)\n",
    "                \n",
    "                # Record TTS start time\n",
    "                tts_start = time.time()\n",
    "                print(\"Converting Marathi text to speech...\")\n",
    "                text_to_speech(marathi_text, 'mr')\n",
    "                tts_time = time.time() - tts_start\n",
    "                \n",
    "                # Calculate processing time\n",
    "                processing_time = time.time() - processing_start\n",
    "                \n",
    "                # Collect timing metrics\n",
    "                processing_times = {\n",
    "                    \"Detection\": detection_time,\n",
    "                    \"Text Processing\": processing_time - translation_time - tts_time,\n",
    "                    \"Translation\": translation_time,\n",
    "                    \"TTS\": tts_time\n",
    "                }\n",
    "                \n",
    "                # Generate and show visualizations\n",
    "                add_visualizations(raw_sentence, processed_sentence, marathi_text, letter_confidences, processing_times)\n",
    "                \n",
    "            else:\n",
    "                print(\"No sign language detected. Please try again.\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            # Modified option 2 with input type and language selection\n",
    "            print(\"\\nSelect Input Type:\")\n",
    "            print(\"1. Text Input\")\n",
    "            print(\"2. Speech Input\")\n",
    "            input_choice = input(\"Enter your choice (1-2): \")\n",
    "            \n",
    "            print(\"\\nSelect Language:\")\n",
    "            print(\"1. English\")\n",
    "            print(\"2. Marathi\")\n",
    "            lang_choice = input(\"Enter your choice (1-2): \")\n",
    "            \n",
    "            # Set language based on user selection\n",
    "            input_language = 'en' if lang_choice == '1' else 'mr'\n",
    "            language_code = 'en-US' if lang_choice == '1' else 'mr-IN'\n",
    "            language_name = 'English' if lang_choice == '1' else 'Marathi'\n",
    "            \n",
    "            # Get input based on user selection\n",
    "            if input_choice == '1':\n",
    "                # Text input\n",
    "                text_input = input(f\"Enter your {language_name} text: \")\n",
    "            else:\n",
    "                # Speech input\n",
    "                text_input = speech_to_text(language_code)\n",
    "                if text_input:\n",
    "                    print(f\"Recognized {language_name} text:\", text_input)\n",
    "                else:\n",
    "                    print(f\"Failed to recognize {language_name} speech. Please try again.\")\n",
    "                    continue\n",
    "            \n",
    "            # Translate to English if input was in Marathi\n",
    "            if input_language == 'mr':\n",
    "                english_text = translate_text(text_input, 'mr', 'en')\n",
    "                print(\"English translation:\", english_text)\n",
    "            else:\n",
    "                english_text = text_input\n",
    "            \n",
    "            # Preserve important words and don't treat them as stopwords\n",
    "            greeting_words = {\"good\", \"morning\", \"hello\", \"hi\", \"afternoon\", \"evening\", \"night\"}\n",
    "            \n",
    "            # Use NLTK for tokenization, normalization, and lemmatization\n",
    "            tokens = word_tokenize(english_text.lower())\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            normalized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "            \n",
    "            # Remove stopwords but preserve greeting words\n",
    "            stop_words = set(stopwords.words('english')) - greeting_words\n",
    "            filtered_tokens = []\n",
    "            \n",
    "            for word in normalized_tokens:\n",
    "                # Keep the word if it's not a stopword or is a greeting word\n",
    "                if word not in stop_words or word.lower() in greeting_words:\n",
    "                    filtered_tokens.append(word)\n",
    "            \n",
    "            normalized_text = ' '.join(filtered_tokens)\n",
    "            \n",
    "            print(f\"Normalized tokens: {filtered_tokens}\")\n",
    "            print(f\"Normalized text for video search: {normalized_text}\")\n",
    "            \n",
    "            dataset_folder = 'youtube_dataset'\n",
    "            output_folder = 'output_video'\n",
    "            \n",
    "            # Try to find and concatenate existing word videos\n",
    "            output_video_path = find_and_concatenate_videos(normalized_text, dataset_folder, output_folder)\n",
    "            if output_video_path:\n",
    "                print(\"Hand sign video generated from existing clips. Displaying video...\")\n",
    "                display(Video(output_video_path, embed=True))\n",
    "            else:\n",
    "                print(\"No existing video clips found. Trying to generate video from raw English text...\")\n",
    "                # Try using the original text directly\n",
    "                output_video_path = find_and_concatenate_videos(english_text, dataset_folder, output_folder)\n",
    "                if output_video_path:\n",
    "                    print(\"Hand sign video generated from raw text. Displaying video...\")\n",
    "                    display(Video(output_video_path, embed=True))\n",
    "                else:\n",
    "                    print(\"Generating video from images...\")\n",
    "                    output_video_path = video_from_image(normalized_text)\n",
    "                    if output_video_path:\n",
    "                        print(\"Hand sign video generated from images. Displaying video...\")\n",
    "                        display(Video(output_video_path, embed=True))\n",
    "                    else:\n",
    "                        print(\"Failed to generate hand sign video from normalized text.\")\n",
    "                        # As a last resort, try to create video from original English text\n",
    "                        output_video_path = video_from_image(english_text)\n",
    "                        if output_video_path:\n",
    "                            print(\"Hand sign video generated from original English text. Displaying video...\")\n",
    "                            display(Video(output_video_path, embed=True))\n",
    "                        else:\n",
    "                            print(\"All methods failed to generate hand sign video.\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            print(\"\\nSelect visualization to view:\")\n",
    "            print(\"1.F1 curve\")\n",
    "            print(\"2.Results\")\n",
    "            print(\"3.R curve\")\n",
    "            print(\"4.Confusion Matrix\")\n",
    "            viz_choice = input(\"Enter your choice (1-4): \")\n",
    "            \n",
    "            # Define paths to the fixed image files\n",
    "            viz_paths = {\n",
    "                '1': r\"C:\\Users\\Piyush\\NLP_mini_Project\\NLP_mini_Project\\train5\\F1_curve.png\",\n",
    "                '2': r\"C:\\Users\\Piyush\\NLP_mini_Project\\NLP_mini_Project\\train5\\results.png\",\n",
    "                '3': r\"C:\\Users\\Piyush\\NLP_mini_Project\\NLP_mini_Project\\train5\\R_curve.png\",\n",
    "                '4': r\"C:\\Users\\Piyush\\NLP_mini_Project\\NLP_mini_Project\\train5\\confusion_matrix_normalized.png\"\n",
    "            }\n",
    "            \n",
    "            if viz_choice in viz_paths:\n",
    "                image_path = viz_paths[viz_choice]\n",
    "                \n",
    "                if os.path.exists(image_path):\n",
    "                    # Read the image\n",
    "                    img = cv2.imread(image_path)\n",
    "                    \n",
    "                    # Get screen dimensions - using a reasonable default for a 1920x1080 screen\n",
    "                    screen_width, screen_height = 1800, 950  # Slightly smaller than full screen to account for window borders\n",
    "                    \n",
    "                    # Get image dimensions\n",
    "                    img_height, img_width = img.shape[:2]\n",
    "                    \n",
    "                    # Calculate scaling factor to fit on screen while maintaining aspect ratio\n",
    "                    scale_factor = min(screen_width / img_width, screen_height / img_height)\n",
    "                    \n",
    "                    # Resize the image\n",
    "                    new_width = int(img_width * scale_factor)\n",
    "                    new_height = int(img_height * scale_factor)\n",
    "                    resized_img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Create a named window that can be resized by the user if needed\n",
    "                    title = \"Sign Language Detection\" if viz_choice == '1' else \"Translation Workflow\"\n",
    "                    cv2.namedWindow(title, cv2.WINDOW_NORMAL)\n",
    "                    \n",
    "                    # Set the initial window size\n",
    "                    cv2.resizeWindow(title, new_width, new_height)\n",
    "                    \n",
    "                    # Show the resized image\n",
    "                    cv2.imshow(title, resized_img)\n",
    "                    \n",
    "                    # Wait for user to press any key\n",
    "                    print(\"Press any key to close the visualization window...\")\n",
    "                    cv2.waitKey(0)\n",
    "                    cv2.destroyAllWindows()\n",
    "                else:\n",
    "                    print(f\"Visualization file not found: {image_path}\")\n",
    "                    print(\"Please ensure the 'visualization_files' directory exists with the required images.\")\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "        \n",
    "        elif choice == '4':\n",
    "            # Create animation of the translation process\n",
    "            text = input(\"Enter text to animate: \")\n",
    "            create_translation_animation(text, output_path=\"translation_animation.mp4\")\n",
    "            print(\"Animation created at translation_animation.mp4\")\n",
    "\n",
    "        elif choice == '5':\n",
    "            print(\"\\nEvaluating system performance metrics...\")\n",
    "            # Normally we would collect real test data, but for demonstration we'll use simulated data\n",
    "            test_data = collect_test_data()\n",
    "            metrics = calculate_system_metrics(test_data)\n",
    "            display_evaluation_metrics(metrics)\n",
    "\n",
    "        elif choice == '6':\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567294db-7ddd-414e-88f8-2cf34ef7ea9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
